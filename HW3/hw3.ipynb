{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Yes/No Questions\n",
    "\n",
    "deadline: 3 декабря 2019, 23:59\n",
    "\n",
    "В этом домашнем задании вы будете работать с корпусом BoolQ. Корпус состоит из вопросов, предполагающих бинарный ответ (да / нет), абзацев из Википедии,  содержащих ответ на вопрос, заголовка статьи, из которой извлечен абзац и непосредственно ответа (true / false).\n",
    "\n",
    "Корпус описан в статье:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "Корпус (train-dev split) доступен в репозитории проекта:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "Используйте для обучения train часть корпуса, для валидации и тестирования – dev часть. \n",
    "\n",
    "Каждый бонус пункт оцениватся в 1 балл. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример вопроса: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРАВИЛА\n",
    "1. Домашнее задание выполняется в группе до 3-х человек.\n",
    "2. Домашнее задание сдается через anytask, инвайты будут дополнительно высланы.\n",
    "3. Домашнее задание оформляется в виде отчета либо в .pdf файле, либо ipython-тетрадке. \n",
    "4. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
    "5. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет.\n",
    "6. Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
    "7. Плагиат и любое недобросоветсное цитирование приводит к обнуление оценки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "DATA_PREF = 'data/'\n",
    "os.makedirs(DATA_PREF, exist_ok=True)\n",
    "TRAIN_FILE_PATH = DATA_PREF + 'train.jsonl'\n",
    "DEV_FILE_PATH = DATA_PREF + 'dev.jsonl'\n",
    "FAST_TEXT_PREF = 'FastText/'\n",
    "os.makedirs(FAST_TEXT_PREF, exist_ok=True)\n",
    "FAST_TEXT_TRAIN_FILE_PATH = FAST_TEXT_PREF + 'train.txt'\n",
    "FAST_TEXT_DEV_FILE_PATH = FAST_TEXT_PREF + 'dev.txt'\n",
    "FAST_TEXT_MODEL_NAME = FAST_TEXT_PREF + 'model.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        df = pd.DataFrame.from_records(list(reader))\n",
    "    df['passage'] = df['passage'].map(unidecode)\n",
    "    df['question'] = df['question'].map(unidecode)\n",
    "    df['title'] = df['title'].map(unidecode)\n",
    "    return df\n",
    "    \n",
    "df_train = open_dataset(TRAIN_FILE_PATH)    \n",
    "df_dev = open_dataset(DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. [1 балл] Эксплоративный анализ\n",
    "1. Посчитайте долю yes и no классов в корпусе\n",
    "2. Оцените среднюю длину вопроса\n",
    "3. Оцените среднюю длину параграфа\n",
    "4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/'pe:rZ@n, -S@n/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is confectionary sugar the same as powdered sugar</td>\n",
       "      <td>Powdered sugar</td>\n",
       "      <td>True</td>\n",
       "      <td>Powdered sugar, also called confectioners' sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is elder scrolls online the same as skyrim</td>\n",
       "      <td>The Elder Scrolls Online</td>\n",
       "      <td>False</td>\n",
       "      <td>As with other games in The Elder Scrolls serie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0    do iran and afghanistan speak the same language   \n",
       "1  do good samaritan laws protect those who help ...   \n",
       "2  is windows movie maker part of windows essentials   \n",
       "3  is confectionary sugar the same as powdered sugar   \n",
       "4         is elder scrolls online the same as skyrim   \n",
       "\n",
       "                      title  answer  \\\n",
       "0          Persian language    True   \n",
       "1        Good Samaritan law    True   \n",
       "2       Windows Movie Maker    True   \n",
       "3            Powdered sugar    True   \n",
       "4  The Elder Scrolls Online   False   \n",
       "\n",
       "                                             passage  \n",
       "0  Persian (/'pe:rZ@n, -S@n/), also known by its ...  \n",
       "1  Good Samaritan laws offer legal protection to ...  \n",
       "2  Windows Movie Maker (formerly known as Windows...  \n",
       "3  Powdered sugar, also called confectioners' sug...  \n",
       "4  As with other games in The Elder Scrolls serie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля True меток:\t\t\t\t 0.6231038506417736\n",
      "Доля False меток:\t\t\t\t 0.37689614935822635\n",
      "Средняя длина вопроса:\t\t\t\t 43.99204412856688 символов\n",
      "Средняя длина параграфа:\t\t\t 565.7658852232948 символов\n",
      "Доля вопросов, начинающихся с ключевых слов: \t 0.9570382942611647\n"
     ]
    }
   ],
   "source": [
    "print('Доля True меток:\\t\\t\\t\\t', (df_train['answer'].values == True).mean())\n",
    "print('Доля False меток:\\t\\t\\t\\t', (df_train['answer'].values == False).mean())\n",
    "len_func = np.vectorize(lambda x: len(x))\n",
    "print('Средняя длина вопроса:\\t\\t\\t\\t', len_func(df_train['question'].values).mean(), 'символов')\n",
    "print('Средняя длина параграфа:\\t\\t\\t', len_func(df_train['passage'].values).mean(), 'символов')\n",
    "yes_no_words_set = set(['did', 'do', 'does', 'is', 'are', 'was', 'were', 'have', 'has', 'can', 'could', 'will', 'would'])\n",
    "is_in_set_func = np.vectorize(lambda x: x.split()[0].lower() in yes_no_words_set)\n",
    "print('Доля вопросов, начинающихся с ключевых слов: \\t', is_in_set_func(df_train['question'].values).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос, задаваемый гуглу, классифицировался как \"yes/no question\", если он начинался с ключевого слова. (слова можно посмотреть в ячейке выше). Соответственно, 95% вопросов датасета начинаются именно с них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. [1 балл] Baseline\n",
    "1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n",
    "2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n",
    "\n",
    "Почему fasttext плохо справляется с этой задачей?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность при выставлении всем меткам True: 0.621713\n"
     ]
    }
   ],
   "source": [
    "y_true = df_dev['answer'].values\n",
    "y_pred = np.ones_like(y_true)\n",
    "print('Точность при выставлении всем меткам True: %f' % (y_true == y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def write_to_fasttext_dataset(df, res_file_path):\n",
    "    f = open(res_file_path, 'w')\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = '__label__' + str(int(obj['answer'])) + ' '\n",
    "        line = line + obj['passage'] + ' ' + obj['question'] + '\\n'\n",
    "        f.write(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NMikhaylov\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d5fc30efaf40f2849a03369de467fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f806bd9ae84d08833e7bd25b128316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_fasttext_dataset(df_train, FAST_TEXT_TRAIN_FILE_PATH)\n",
    "write_to_fasttext_dataset(df_dev, FAST_TEXT_DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found backup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_fasttext_model():\n",
    "    if os.path.exists(FAST_TEXT_MODEL_NAME) and os.path.isfile(FAST_TEXT_MODEL_NAME):\n",
    "        print('Found backup')\n",
    "        return fasttext.load_model(FAST_TEXT_MODEL_NAME)\n",
    "    model = fasttext.train_supervised(FAST_TEXT_TRAIN_FILE_PATH, label_prefix='__label__', epoch=400)\n",
    "    model.save_model(FAST_TEXT_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "fasttext_model = train_fasttext_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NMikhaylov\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cb3fdc061d444080d28a7408196433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_input_fasttext(df):\n",
    "    inp = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = obj['passage'] + ' ' + obj['question']\n",
    "        inp.append(line)\n",
    "    return inp\n",
    "dev_fasttext_input = make_input_fasttext(df_dev)\n",
    "fasttext_prediction = fasttext_model.predict(dev_fasttext_input)[0]\n",
    "fasttext_prediction = np.array([int(s[0][len('__label__'):]) for s in fasttext_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513761467889908\n"
     ]
    }
   ],
   "source": [
    "print((fasttext_prediction == y_true).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText плохо справился, потому что классы несбалансированны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [1 балл] Используем эмбеддинги предложений\n",
    "1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n",
    "\n",
    "[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n",
    "\n",
    "[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
    "          (RobertaModel,    RobertaTokenizer,    'roberta-base')]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vector(sentence, tokenizer):\n",
    "    tokenized_sent = tokenizer.encode(sentence, add_special_tokens=True, max_length=256)\n",
    "    return torch.LongTensor(tokenized_sent)\n",
    "\n",
    "def sentences2tensor(sentences, tokenizer):\n",
    "    vectors = [sentence2vector(sent, tokenizer) for sent in sentences]\n",
    "    lens = [len(vec) for vec in vectors]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    matrix = torch.zeros((len(vectors), max_len), dtype=torch.long)\n",
    "    mask = torch.zeros_like(matrix)\n",
    "    \n",
    "    for i, cur_len in enumerate(lens):\n",
    "        matrix[i, :cur_len] = vectors[i]\n",
    "        mask[i, :cur_len] = 1.\n",
    "    \n",
    "    return matrix, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2079, 4238, 1998, 7041, 3713, 1996, 2168, 2653,  102,    0,    0,\n",
       "             0,    0,    0],\n",
       "         [ 101, 2079, 2204, 3520, 8486, 5794, 4277, 4047, 2216, 2040, 2393, 2012,\n",
       "          2019, 4926,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence2vector(df_train['question'][0], BertTokenizer.from_pretrained(MODELS[0][2]), 10)\n",
    "sentences2tensor(df_train['question'][0:2], BertTokenizer.from_pretrained(MODELS[0][2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a11714e9707483f893eab91337bd6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b91c6f90be94f26b7cb4938cb675b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.45      0.50      1237\n",
      "        True       0.70      0.77      0.73      2033\n",
      "\n",
      "    accuracy                           0.65      3270\n",
      "   macro avg       0.62      0.61      0.62      3270\n",
      "weighted avg       0.64      0.65      0.64      3270\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d08469260864fa48e2ed96a991b48cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39451ae952ac48a397f512ec90c1393c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.41      0.48      1237\n",
      "        True       0.70      0.82      0.75      2033\n",
      "\n",
      "    accuracy                           0.67      3270\n",
      "   macro avg       0.64      0.62      0.62      3270\n",
      "weighted avg       0.65      0.67      0.65      3270\n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "bert_emb_size = 768\n",
    "\n",
    "def calc_embeddings(df, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        question_embeddings = torch.zeros((df.shape[0], bert_emb_size))\n",
    "        passage_embeddings = torch.zeros((df.shape[0], bert_emb_size))\n",
    "        bar = tqdm(total=df.shape[0])\n",
    "        for begin in range(0, df.shape[0], batch_size):\n",
    "            end = begin + batch_size\n",
    "            if end >= df.shape[0]:\n",
    "                end = df.shape[0]\n",
    "            tokenized, mask = sentences2tensor(df['question'].values[begin:end], tokenizer)\n",
    "            batch_embedding, _ = model(tokenized.cuda(), mask.cuda())\n",
    "            #print(batch_embedding.shape)\n",
    "            batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "            #print(batch_embedding.shape)\n",
    "            question_embeddings[begin:end] = batch_embedding\n",
    "\n",
    "            tokenized, mask = sentences2tensor(df['passage'].values[begin:end], tokenizer)\n",
    "            batch_embedding, _ = model(tokenized.cuda(), mask.cuda())\n",
    "            batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "            passage_embeddings[begin:end] = batch_embedding\n",
    "            bar.update(end - begin)\n",
    "    \n",
    "    embed = torch.cat((question_embeddings, passage_embeddings), dim=-1)\n",
    "    #print(embed.shape)\n",
    "    return embed\n",
    "        \n",
    "\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    print(pretrained_weights)\n",
    "    \n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "    \n",
    "    x_train = calc_embeddings(df_train, model, tokenizer)\n",
    "    y_train = df_train['answer']\n",
    "    x_test = calc_embeddings(df_dev, model, tokenizer)\n",
    "    y_test = df_dev['answer']\n",
    "    log_reg = LogisticRegression().fit(x_train, y_train)\n",
    "    y_pred = log_reg.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    model = model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итого мы попробовали классикечкий BERT и roBERTa. Как видим по F-мере получилось, что roBERTa лучше справлвяется с задачей, но всего лишь на 0.02 и то если выставить всем меткам True, то будет лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [3 балла] DrQA-подобная архитектура\n",
    "\n",
    "Основана на статье: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "Архитектура DrQA предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик абзаца [paragraph encoding] – LSTM, получаящая на вход вектора слов, состоящие из: \n",
    "* эмбеддинга слова (w2v или fasttext)\n",
    "* дополнительных признаков-индикаторов, кодирующих в виде one-hot векторов часть речи слова, является ли оно именованной сущностью или нет, встречается ли слово в вопросе или нет \n",
    "* выровненного эмбеддинга вопроса, получаемого с использованием soft attention между эмбеддингами слов из абзаца и эмбеддингом вопроса.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_j􏰂 a_{i,j} E(q_j)$, где $E(q_j)$ – эмбеддинг слова из вопроса. Формула для $a_{i,j}$ приведена в статье. \n",
    "\n",
    "2. Кодировщик вопроса [question encoding] – LSTM, получаящая на вход эмбеддинги слов из вопроса. Выход кодировщика: $q = 􏰂\\sum_j􏰂  b_j q_j$. Формула для $b_{j}$ приведена в статье. \n",
    "\n",
    "3. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре DrQA, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentance(df):\n",
    "    df['passage'] = df['passage'].map(unidecode)\n",
    "    df['question'] = df['question'].map(unidecode)\n",
    "    df['title'] = df['title'].map(unidecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. [3 балла] BiDAF-подобная архитектура\n",
    "\n",
    "Основана на статье: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "Архитектура BiDAF предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик  получает на вход два представления слова: эмбеддинг слова и полученное из CNN посимвольное представление слова. Кодировщики для вопроса и для параграфа одинаковы. \n",
    "2. Слой внимания (детальное описание приведено в статье, см. пункт Attention Flow Layer)\n",
    "3. Промежуточный слой, который получает на вход контекстуализированные эмбеддинги слов из параграфа, состоящие из трех частей (выход кодировщика параграфа,   Query2Context (один вектор) и Context2Query (матрица) выравнивания\n",
    "\n",
    "4. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре BiDAF, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение DrQA и BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. [1 балл] Итоги\n",
    "Напишите краткое резюме проделанной работы. Сравните результаты всех разработанных моделей. Что помогло вам в выполнении работы, чего не хватало?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
