{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Yes/No Questions\n",
    "\n",
    "deadline: 3 декабря 2019, 23:59\n",
    "\n",
    "В этом домашнем задании вы будете работать с корпусом BoolQ. Корпус состоит из вопросов, предполагающих бинарный ответ (да / нет), абзацев из Википедии,  содержащих ответ на вопрос, заголовка статьи, из которой извлечен абзац и непосредственно ответа (true / false).\n",
    "\n",
    "Корпус описан в статье:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "Корпус (train-dev split) доступен в репозитории проекта:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "Используйте для обучения train часть корпуса, для валидации и тестирования – dev часть. \n",
    "\n",
    "Каждый бонус пункт оцениватся в 1 балл. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример вопроса: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРАВИЛА\n",
    "1. Домашнее задание выполняется в группе до 3-х человек.\n",
    "2. Домашнее задание сдается через anytask, инвайты будут дополнительно высланы.\n",
    "3. Домашнее задание оформляется в виде отчета либо в .pdf файле, либо ipython-тетрадке. \n",
    "4. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
    "5. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет.\n",
    "6. Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
    "7. Плагиат и любое недобросоветсное цитирование приводит к обнуление оценки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_PREF = 'data/'\n",
    "os.makedirs(DATA_PREF, exist_ok=True)\n",
    "TRAIN_FILE_PATH = DATA_PREF + 'train.jsonl'\n",
    "DEV_FILE_PATH = DATA_PREF + 'dev.jsonl'\n",
    "FAST_TEXT_PREF = 'FastText/'\n",
    "os.makedirs(FAST_TEXT_PREF, exist_ok=True)\n",
    "FAST_TEXT_TRAIN_FILE_PATH = FAST_TEXT_PREF + 'train.txt'\n",
    "FAST_TEXT_DEV_FILE_PATH = FAST_TEXT_PREF + 'dev.txt'\n",
    "FAST_TEXT_MODEL_NAME = FAST_TEXT_PREF + 'model.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        df = pd.DataFrame.from_records(list(reader))\n",
    "    df['passage'] = df['passage'].map(unidecode)\n",
    "    df['question'] = df['question'].map(unidecode)\n",
    "    df['title'] = df['title'].map(unidecode)\n",
    "    return df\n",
    "    \n",
    "df_train = open_dataset(TRAIN_FILE_PATH)    \n",
    "df_dev = open_dataset(DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. [1 балл] Эксплоративный анализ\n",
    "1. Посчитайте долю yes и no классов в корпусе\n",
    "2. Оцените среднюю длину вопроса\n",
    "3. Оцените среднюю длину параграфа\n",
    "4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/'pe:rZ@n, -S@n/), also known by its ...</td>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>Powdered sugar, also called confectioners' sug...</td>\n",
       "      <td>is confectionary sugar the same as powdered sugar</td>\n",
       "      <td>Powdered sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>As with other games in The Elder Scrolls serie...</td>\n",
       "      <td>is elder scrolls online the same as skyrim</td>\n",
       "      <td>The Elder Scrolls Online</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer                                            passage  \\\n",
       "0    True  Persian (/'pe:rZ@n, -S@n/), also known by its ...   \n",
       "1    True  Good Samaritan laws offer legal protection to ...   \n",
       "2    True  Windows Movie Maker (formerly known as Windows...   \n",
       "3    True  Powdered sugar, also called confectioners' sug...   \n",
       "4   False  As with other games in The Elder Scrolls serie...   \n",
       "\n",
       "                                            question                     title  \n",
       "0    do iran and afghanistan speak the same language          Persian language  \n",
       "1  do good samaritan laws protect those who help ...        Good Samaritan law  \n",
       "2  is windows movie maker part of windows essentials       Windows Movie Maker  \n",
       "3  is confectionary sugar the same as powdered sugar            Powdered sugar  \n",
       "4         is elder scrolls online the same as skyrim  The Elder Scrolls Online  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля True меток:\t\t\t\t 0.6231038506417736\n",
      "Доля False меток:\t\t\t\t 0.37689614935822635\n",
      "Средняя длина вопроса:\t\t\t\t 43.99204412856688 символов\n",
      "Средняя длина параграфа:\t\t\t 565.7658852232948 символов\n",
      "Доля вопросов, начинающихся с ключевых слов: \t 0.9570382942611647\n"
     ]
    }
   ],
   "source": [
    "print('Доля True меток:\\t\\t\\t\\t', (df_train['answer'].values == True).mean())\n",
    "print('Доля False меток:\\t\\t\\t\\t', (df_train['answer'].values == False).mean())\n",
    "len_func = np.vectorize(lambda x: len(x))\n",
    "print('Средняя длина вопроса:\\t\\t\\t\\t', len_func(df_train['question'].values).mean(), 'символов')\n",
    "print('Средняя длина параграфа:\\t\\t\\t', len_func(df_train['passage'].values).mean(), 'символов')\n",
    "yes_no_words_set = set(['did', 'do', 'does', 'is', 'are', 'was', 'were', 'have', 'has', 'can', 'could', 'will', 'would'])\n",
    "is_in_set_func = np.vectorize(lambda x: x.split()[0].lower() in yes_no_words_set)\n",
    "print('Доля вопросов, начинающихся с ключевых слов: \\t', is_in_set_func(df_train['question'].values).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос, задаваемый гуглу, классифицировался как \"yes/no question\", если он начинался с ключевого слова. (слова можно посмотреть в ячейке выше). Соответственно, 95% вопросов датасета начинаются именно с них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. [1 балл] Baseline\n",
    "1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n",
    "2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n",
    "\n",
    "Почему fasttext плохо справляется с этой задачей?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность при выставлении всем меткам True: 0.621713\n"
     ]
    }
   ],
   "source": [
    "y_true = df_dev['answer'].values\n",
    "y_pred = np.ones_like(y_true)\n",
    "print('Точность при выставлении всем меткам True: %f' % (y_true == y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def write_to_fasttext_dataset(df, res_file_path):\n",
    "    f = open(res_file_path, 'w')\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = '__label__' + str(int(obj['answer'])) + ' '\n",
    "        line = line + obj['passage'] + ' ' + obj['question'] + '\\n'\n",
    "        f.write(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53dff36824242e692f86d02cd45b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9427), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d186cc5d68b241048cdf81ffe541d118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3270), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_fasttext_dataset(df_train, FAST_TEXT_TRAIN_FILE_PATH)\n",
    "write_to_fasttext_dataset(df_dev, FAST_TEXT_DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found backup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_fasttext_model():\n",
    "    if os.path.exists(FAST_TEXT_MODEL_NAME) and os.path.isfile(FAST_TEXT_MODEL_NAME):\n",
    "        print('Found backup')\n",
    "        return fasttext.load_model(FAST_TEXT_MODEL_NAME)\n",
    "    model = fasttext.train_supervised(FAST_TEXT_TRAIN_FILE_PATH, label_prefix='__label__', epoch=400)\n",
    "    model.save_model(FAST_TEXT_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "fasttext_model = train_fasttext_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e60b2877a340bba28141344d9ecf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3270), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_input_fasttext(df):\n",
    "    inp = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = obj['passage'] + ' ' + obj['question']\n",
    "        inp.append(line)\n",
    "    return inp\n",
    "dev_fasttext_input = make_input_fasttext(df_dev)\n",
    "fasttext_prediction = fasttext_model.predict(dev_fasttext_input)[0]\n",
    "fasttext_prediction = np.array([int(s[0][len('__label__'):]) for s in fasttext_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513761467889908\n"
     ]
    }
   ],
   "source": [
    "print((fasttext_prediction == y_true).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText плохо справился, потому что классы несбалансированны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [1 балл] Используем эмбеддинги предложений\n",
    "1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n",
    "\n",
    "[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n",
    "\n",
    "[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [3 балла] DrQA-подобная архитектура\n",
    "\n",
    "Основана на статье: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "Архитектура DrQA предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик абзаца [paragraph encoding] – LSTM, получаящая на вход вектора слов, состоящие из: \n",
    "* эмбеддинга слова (w2v или fasttext)\n",
    "* дополнительных признаков-индикаторов, кодирующих в виде one-hot векторов часть речи слова, является ли оно именованной сущностью или нет, встречается ли слово в вопросе или нет \n",
    "* выровненного эмбеддинга вопроса, получаемого с использованием soft attention между эмбеддингами слов из абзаца и эмбеддингом вопроса.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_j􏰂 a_{i,j} E(q_j)$, где $E(q_j)$ – эмбеддинг слова из вопроса. Формула для $a_{i,j}$ приведена в статье. \n",
    "\n",
    "2. Кодировщик вопроса [question encoding] – LSTM, получаящая на вход эмбеддинги слов из вопроса. Выход кодировщика: $q = 􏰂\\sum_j􏰂  b_j q_j$. Формула для $b_{j}$ приведена в статье. \n",
    "\n",
    "3. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре DrQA, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. [3 балла] BiDAF-подобная архитектура\n",
    "\n",
    "Основана на статье: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "Архитектура BiDAF предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик  получает на вход два представления слова: эмбеддинг слова и полученное из CNN посимвольное представление слова. Кодировщики для вопроса и для параграфа одинаковы. \n",
    "2. Слой внимания (детальное описание приведено в статье, см. пункт Attention Flow Layer)\n",
    "3. Промежуточный слой, который получает на вход контекстуализированные эмбеддинги слов из параграфа, состоящие из трех частей (выход кодировщика параграфа,   Query2Context (один вектор) и Context2Query (матрица) выравнивания\n",
    "\n",
    "4. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре BiDAF, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение DrQA и BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. [1 балл] Итоги\n",
    "Напишите краткое резюме проделанной работы. Сравните результаты всех разработанных моделей. Что помогло вам в выполнении работы, чего не хватало?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
