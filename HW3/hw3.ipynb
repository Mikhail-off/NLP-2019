{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 3\n",
    "## Yes/No Questions\n",
    "\n",
    "deadline: 3 –¥–µ–∫–∞–±—Ä—è 2019, 23:59\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–æ—Ä–ø—É—Å–æ–º BoolQ. –ö–æ—Ä–ø—É—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—â–∏—Ö –±–∏–Ω–∞—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–∞ / –Ω–µ—Ç), –∞–±–∑–∞—Ü–µ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏,  —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∏–∑–≤–ª–µ—á–µ–Ω –∞–±–∑–∞—Ü –∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–≤–µ—Ç–∞ (true / false).\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å (train-dev split) –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è train —á–∞—Å—Ç—å –∫–æ—Ä–ø—É—Å–∞, –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì dev —á–∞—Å—Ç—å. \n",
    "\n",
    "–ö–∞–∂–¥—ã–π –±–æ–Ω—É—Å –ø—É–Ω–∫—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—Å—è –≤ 1 –±–∞–ª–ª. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–†–ê–í–ò–õ–ê\n",
    "1. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –≥—Ä—É–ø–ø–µ –¥–æ 3-—Ö —á–µ–ª–æ–≤–µ–∫.\n",
    "2. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ —Å–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ anytask, –∏–Ω–≤–∞–π—Ç—ã –±—É–¥—É—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–ª–∞–Ω—ã.\n",
    "3. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –æ—Ç—á–µ—Ç–∞ –ª–∏–±–æ –≤ .pdf —Ñ–∞–π–ª–µ, –ª–∏–±–æ ipython-—Ç–µ—Ç—Ä–∞–¥–∫–µ. \n",
    "4. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: –Ω—É–º–µ—Ä–∞—Ü–∏—é –∑–∞–¥–∞–Ω–∏–π –∏ –ø—É–Ω–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏, –∫–æ–¥ —Ä–µ—à–µ–Ω–∏—è, –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã —Å–¥–µ–ª–∞–ª–∏. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∏–∑–ª–∏—à–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–µ–Ω–≥–∞ –∏ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –Ω–æ—Ä–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "5. –ù–µ —Å—Ç–æ–∏—Ç –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ª–µ–∫—Ü–∏–π, —Å—Ç–∞—Ç–µ–π –∏ –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤ –≤–∞—à –æ—Ç—á–µ—Ç.\n",
    "6. –û—Ç—á–µ—Ç—ã, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑ –∫–æ–¥–∞, –Ω–µ –±—É–¥—É—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–µ–Ω—ã –Ω—É–ª–µ–≤–æ–π –æ—Ü–µ–Ω–∫–æ–π.\n",
    "7. –ü–ª–∞–≥–∏–∞—Ç –∏ –ª—é–±–æ–µ –Ω–µ–¥–æ–±—Ä–æ—Å–æ–≤–µ—Ç—Å–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ–±–Ω—É–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "DATA_PREF = 'data/'\n",
    "os.makedirs(DATA_PREF, exist_ok=True)\n",
    "TRAIN_FILE_PATH = DATA_PREF + 'train.jsonl'\n",
    "DEV_FILE_PATH = DATA_PREF + 'dev.jsonl'\n",
    "FAST_TEXT_PREF = 'FastText/'\n",
    "os.makedirs(FAST_TEXT_PREF, exist_ok=True)\n",
    "FAST_TEXT_TRAIN_FILE_PATH = FAST_TEXT_PREF + 'train.txt'\n",
    "FAST_TEXT_DEV_FILE_PATH = FAST_TEXT_PREF + 'dev.txt'\n",
    "FAST_TEXT_MODEL_NAME = FAST_TEXT_PREF + 'model.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        df = pd.DataFrame.from_records(list(reader))\n",
    "    df['passage'] = df['passage'].map(unidecode)\n",
    "    df['question'] = df['question'].map(unidecode)\n",
    "    df['title'] = df['title'].map(unidecode)\n",
    "    return df\n",
    "    \n",
    "df_train = open_dataset(TRAIN_FILE_PATH)    \n",
    "df_dev = open_dataset(DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 1. [1 –±–∞–ª–ª] –≠–∫—Å–ø–ª–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "1. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é yes –∏ no –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –≤–æ–ø—Ä–æ—Å–∞\n",
    "3. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞\n",
    "4. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ, –ø–æ –∫–∞–∫–∏–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –≤–æ–ø—Ä–æ—Å—ã (–∏–ª–∏ –Ω–∞–π–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∞—Ç—å–µ). –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—É–π—Ç–µ, –∫–∞–∫ —ç—Ç–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ—Ä–ø—É—Å–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/'pe:rZ@n, -S@n/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is confectionary sugar the same as powdered sugar</td>\n",
       "      <td>Powdered sugar</td>\n",
       "      <td>True</td>\n",
       "      <td>Powdered sugar, also called confectioners' sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is elder scrolls online the same as skyrim</td>\n",
       "      <td>The Elder Scrolls Online</td>\n",
       "      <td>False</td>\n",
       "      <td>As with other games in The Elder Scrolls serie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0    do iran and afghanistan speak the same language   \n",
       "1  do good samaritan laws protect those who help ...   \n",
       "2  is windows movie maker part of windows essentials   \n",
       "3  is confectionary sugar the same as powdered sugar   \n",
       "4         is elder scrolls online the same as skyrim   \n",
       "\n",
       "                      title  answer  \\\n",
       "0          Persian language    True   \n",
       "1        Good Samaritan law    True   \n",
       "2       Windows Movie Maker    True   \n",
       "3            Powdered sugar    True   \n",
       "4  The Elder Scrolls Online   False   \n",
       "\n",
       "                                             passage  \n",
       "0  Persian (/'pe:rZ@n, -S@n/), also known by its ...  \n",
       "1  Good Samaritan laws offer legal protection to ...  \n",
       "2  Windows Movie Maker (formerly known as Windows...  \n",
       "3  Powdered sugar, also called confectioners' sug...  \n",
       "4  As with other games in The Elder Scrolls serie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–æ–ª—è True –º–µ—Ç–æ–∫:\t\t\t\t 0.6231038506417736\n",
      "–î–æ–ª—è False –º–µ—Ç–æ–∫:\t\t\t\t 0.37689614935822635\n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞:\t\t\t\t 43.99204412856688 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞:\t\t\t 565.7658852232948 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "–î–æ–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: \t 0.9570382942611647\n"
     ]
    }
   ],
   "source": [
    "print('–î–æ–ª—è True –º–µ—Ç–æ–∫:\\t\\t\\t\\t', (df_train['answer'].values == True).mean())\n",
    "print('–î–æ–ª—è False –º–µ—Ç–æ–∫:\\t\\t\\t\\t', (df_train['answer'].values == False).mean())\n",
    "len_func = np.vectorize(lambda x: len(x))\n",
    "print('–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞:\\t\\t\\t\\t', len_func(df_train['question'].values).mean(), '—Å–∏–º–≤–æ–ª–æ–≤')\n",
    "print('–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞:\\t\\t\\t', len_func(df_train['passage'].values).mean(), '—Å–∏–º–≤–æ–ª–æ–≤')\n",
    "yes_no_words_set = set(['did', 'do', 'does', 'is', 'are', 'was', 'were', 'have', 'has', 'can', 'could', 'will', 'would'])\n",
    "is_in_set_func = np.vectorize(lambda x: x.split()[0].lower() in yes_no_words_set)\n",
    "print('–î–æ–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: \\t', is_in_set_func(df_train['question'].values).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–ø—Ä–æ—Å, –∑–∞–¥–∞–≤–∞–µ–º—ã–π –≥—É–≥–ª—É, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª—Å—è –∫–∞–∫ \"yes/no question\", –µ—Å–ª–∏ –æ–Ω –Ω–∞—á–∏–Ω–∞–ª—Å—è —Å –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞. (—Å–ª–æ–≤–∞ –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤ —è—á–µ–π–∫–µ –≤—ã—à–µ). –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, 95% –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –∏–º–µ–Ω–Ω–æ —Å –Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 2. [1 –±–∞–ª–ª] Baseline\n",
    "1. –û—Ü–µ–Ω–∏—Ç–µ accuracy —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–≤—Å–µ–º –ø—Ä–æ—Å—Ç–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: –ø—Ä–∏—Å–≤–æ–∏—Ç—å –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ dev —á–∞—Å—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å –∏–∑ train —á–∞—Å—Ç–∏\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ accuracy —á—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: fasttext –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–∫–ª–µ–µ–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∞–±–∑–∞—Ü–µ–≤ (' '.join([question, passage]))\n",
    "\n",
    "–ü–æ—á–µ–º—É fasttext –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –≤—Å–µ–º –º–µ—Ç–∫–∞–º True: 0.621713\n"
     ]
    }
   ],
   "source": [
    "y_true = df_dev['answer'].values\n",
    "y_pred = np.ones_like(y_true)\n",
    "print('–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –≤—Å–µ–º –º–µ—Ç–∫–∞–º True: %f' % (y_true == y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def write_to_fasttext_dataset(df, res_file_path):\n",
    "    f = open(res_file_path, 'w')\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = '__label__' + str(int(obj['answer'])) + ' '\n",
    "        line = line + obj['passage'] + ' ' + obj['question'] + '\\n'\n",
    "        f.write(line)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NMikhaylov\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d5fc30efaf40f2849a03369de467fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f806bd9ae84d08833e7bd25b128316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_fasttext_dataset(df_train, FAST_TEXT_TRAIN_FILE_PATH)\n",
    "write_to_fasttext_dataset(df_dev, FAST_TEXT_DEV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found backup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_fasttext_model():\n",
    "    if os.path.exists(FAST_TEXT_MODEL_NAME) and os.path.isfile(FAST_TEXT_MODEL_NAME):\n",
    "        print('Found backup')\n",
    "        return fasttext.load_model(FAST_TEXT_MODEL_NAME)\n",
    "    model = fasttext.train_supervised(FAST_TEXT_TRAIN_FILE_PATH, label_prefix='__label__', epoch=400)\n",
    "    model.save_model(FAST_TEXT_MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "fasttext_model = train_fasttext_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NMikhaylov\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cb3fdc061d444080d28a7408196433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_input_fasttext(df):\n",
    "    inp = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        obj = df.iloc[i]\n",
    "        line = obj['passage'] + ' ' + obj['question']\n",
    "        inp.append(line)\n",
    "    return inp\n",
    "dev_fasttext_input = make_input_fasttext(df_dev)\n",
    "fasttext_prediction = fasttext_model.predict(dev_fasttext_input)[0]\n",
    "fasttext_prediction = np.array([int(s[0][len('__label__'):]) for s in fasttext_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513761467889908\n"
     ]
    }
   ],
   "source": [
    "print((fasttext_prediction == y_true).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–∏–ª—Å—è, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–ª–∞—Å—Å—ã –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [1 –±–∞–ª–ª] –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞ –∏ –æ—Ü–µ–Ω–∏—Ç–µ accuracy —ç—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. \n",
    "\n",
    "[bonus] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers. –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–∞—Å—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã?\n",
    "\n",
    "[bonus] –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –º–µ—Ç–æ–¥ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),\n",
    "          (RobertaModel,    RobertaTokenizer,    'roberta-base')]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vector(sentence, tokenizer):\n",
    "    tokenized_sent = tokenizer.encode(sentence, add_special_tokens=True, max_length=256)\n",
    "    return torch.LongTensor(tokenized_sent)\n",
    "\n",
    "def sentences2tensor(sentences, tokenizer):\n",
    "    vectors = [sentence2vector(sent, tokenizer) for sent in sentences]\n",
    "    lens = [len(vec) for vec in vectors]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    matrix = torch.zeros((len(vectors), max_len), dtype=torch.long)\n",
    "    mask = torch.zeros_like(matrix)\n",
    "    \n",
    "    for i, cur_len in enumerate(lens):\n",
    "        matrix[i, :cur_len] = vectors[i]\n",
    "        mask[i, :cur_len] = 1.\n",
    "    \n",
    "    return matrix, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2079, 4238, 1998, 7041, 3713, 1996, 2168, 2653,  102,    0,    0,\n",
       "             0,    0,    0],\n",
       "         [ 101, 2079, 2204, 3520, 8486, 5794, 4277, 4047, 2216, 2040, 2393, 2012,\n",
       "          2019, 4926,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence2vector(df_train['question'][0], BertTokenizer.from_pretrained(MODELS[0][2]), 10)\n",
    "sentences2tensor(df_train['question'][0:2], BertTokenizer.from_pretrained(MODELS[0][2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a11714e9707483f893eab91337bd6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b91c6f90be94f26b7cb4938cb675b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.45      0.50      1237\n",
      "        True       0.70      0.77      0.73      2033\n",
      "\n",
      "    accuracy                           0.65      3270\n",
      "   macro avg       0.62      0.61      0.62      3270\n",
      "weighted avg       0.64      0.65      0.64      3270\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d08469260864fa48e2ed96a991b48cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9427.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39451ae952ac48a397f512ec90c1393c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3270.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.41      0.48      1237\n",
      "        True       0.70      0.82      0.75      2033\n",
      "\n",
      "    accuracy                           0.67      3270\n",
      "   macro avg       0.64      0.62      0.62      3270\n",
      "weighted avg       0.65      0.67      0.65      3270\n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "bert_emb_size = 768\n",
    "\n",
    "def calc_embeddings(df, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        question_embeddings = torch.zeros((df.shape[0], bert_emb_size))\n",
    "        passage_embeddings = torch.zeros((df.shape[0], bert_emb_size))\n",
    "        bar = tqdm(total=df.shape[0])\n",
    "        for begin in range(0, df.shape[0], batch_size):\n",
    "            end = begin + batch_size\n",
    "            if end >= df.shape[0]:\n",
    "                end = df.shape[0]\n",
    "            tokenized, mask = sentences2tensor(df['question'].values[begin:end], tokenizer)\n",
    "            batch_embedding, _ = model(tokenized.cuda(), mask.cuda())\n",
    "            #print(batch_embedding.shape)\n",
    "            batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "            #print(batch_embedding.shape)\n",
    "            question_embeddings[begin:end] = batch_embedding\n",
    "\n",
    "            tokenized, mask = sentences2tensor(df['passage'].values[begin:end], tokenizer)\n",
    "            batch_embedding, _ = model(tokenized.cuda(), mask.cuda())\n",
    "            batch_embedding = torch.mean(batch_embedding, dim=1)\n",
    "            passage_embeddings[begin:end] = batch_embedding\n",
    "            bar.update(end - begin)\n",
    "    \n",
    "    embed = torch.cat((question_embeddings, passage_embeddings), dim=-1)\n",
    "    #print(embed.shape)\n",
    "    return embed\n",
    "        \n",
    "\n",
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    print(pretrained_weights)\n",
    "    \n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    model = model.cuda()\n",
    "    model = model.eval()\n",
    "    \n",
    "    x_train = calc_embeddings(df_train, model, tokenizer)\n",
    "    y_train = df_train['answer']\n",
    "    x_test = calc_embeddings(df_dev, model, tokenizer)\n",
    "    y_test = df_dev['answer']\n",
    "    log_reg = LogisticRegression().fit(x_train, y_train)\n",
    "    y_pred = log_reg.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    model = model.cpu()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥–æ –º—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∏ –∫–ª–∞—Å—Å–∏–∫–µ—á–∫–∏–π BERT –∏ roBERTa. –ö–∞–∫ –≤–∏–¥–∏–º –ø–æ F-–º–µ—Ä–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, —á—Ç–æ roBERTa –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª–≤—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π, –Ω–æ –≤—Å–µ–≥–æ –ª–∏—à—å –Ω–∞ 0.02 –∏ —Ç–æ –µ—Å–ª–∏ –≤—ã—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ–º –º–µ—Ç–∫–∞–º True, —Ç–æ –±—É–¥–µ—Ç –ª—É—á—à–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [3 –±–∞–ª–ª–∞] DrQA-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DrQA –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∞–±–∑–∞—Ü–∞ [paragraph encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑: \n",
    "* —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ (w2v –∏–ª–∏ fasttext)\n",
    "* –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –≤ –≤–∏–¥–µ one-hot –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–∞—Å—Ç—å —Ä–µ—á–∏ —Å–ª–æ–≤–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é –∏–ª–∏ –Ω–µ—Ç, –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏–ª–∏ –Ω–µ—Ç \n",
    "* –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞, –ø–æ–ª—É—á–∞–µ–º–æ–≥–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º soft attention –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å–ª–æ–≤ –∏–∑ –∞–±–∑–∞—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –≤–æ–ø—Ä–æ—Å–∞.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_jÙè∞Ç a_{i,j} E(q_j)$, –≥–¥–µ $E(q_j)$ ‚Äì —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $a_{i,j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "2. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –≤–æ–ø—Ä–æ—Å–∞ [question encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –í—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞: $q = Ùè∞Ç\\sum_jÙè∞Ç  b_j q_j$. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $b_{j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "3. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ DrQA, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentance(df):\n",
    "    df['passage'] = df['passage'].map(unidecode)\n",
    "    df['question'] = df['question'].map(unidecode)\n",
    "    df['title'] = df['title'].map(unidecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 4. [3 –±–∞–ª–ª–∞] BiDAF-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BiDAF –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫  –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –¥–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∏–∑ CNN –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã. \n",
    "2. –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è (–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –≤ —Å—Ç–∞—Ç—å–µ, —Å–º. –ø—É–Ω–∫—Ç Attention Flow Layer)\n",
    "3. –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ —Ç—Ä–µ—Ö —á–∞—Å—Ç–µ–π (–≤—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞,   Query2Context (–æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä) –∏ Context2Query (–º–∞—Ç—Ä–∏—Ü–∞) –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "\n",
    "4. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ BiDAF, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ DrQA –∏ BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 5. [1 –±–∞–ª–ª] –ò—Ç–æ–≥–∏\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ß—Ç–æ –ø–æ–º–æ–≥–ª–æ –≤–∞–º –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
