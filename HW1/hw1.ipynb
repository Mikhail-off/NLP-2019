{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1\n",
    "## Harry Potter and the Action Prediction Challenge from Natural Language\n",
    "\n",
    "*deadline*: 2 октября 2019, 23:59\n",
    "\n",
    "В этом домашнем задании вы будете работать с корпусом Harry Potter and the Action Prediction Challenge. Корпус собран из фанфиков о Гарри Поттере и состоит из двух частей: 1) сырые тексты, 2) фрагменты текстов, описывающих ситуацию, в которой произнесено заклинание.\n",
    "\n",
    "Корпус описан в статье: https://arxiv.org/pdf/1905.11037.pdf\n",
    "\n",
    "David Vilares and Carlos Gómez-Rodríguez. Harry Potter and the Action Prediction Challenge from Natural Language. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics. To appear.\n",
    "\n",
    "Код для сбора корпуса находится в репозитории: https://github.com/aghie/hpac . Корпус можно скачать по инструкции из этого репозитория, но для экономии времени авторы задания уже скачали и подготовили данные к работе. \n",
    "\n",
    "Ссылки на собранный корпус: \n",
    "* Сырые тексты:  https://www.dropbox.com/s/23xet9kvbqna1qs/hpac_raw.zip?dl=0\n",
    "* Токенизированные тексты в нижнем регистре: https://www.dropbox.com/s/gwfgmomdbetvdye/hpac_lower_tokenized.zip?dl=0\n",
    "* train-test-dev: https://www.dropbox.com/s/3vdz0mouvex8abd/hpac_splits.zip?dl=0\n",
    "\n",
    "Части 1, 2 задания должны быть выполнены на полных текстах (сырых или предобработанных -- на ваше усмотрение), Часть 3 – на разбиение на тестовое, отладочное и обучающее множества. Тестовое множество должно быть использовано исключительно для тестирования моделей, обучающее и отладочное – для выбора модели и параметров. \n",
    "\n",
    "В статье и репозитории вы найдете идеи, которые помогут вам выполнить домашнее задание. Их стоит воспринимать как руководство к действию, и не стоит их копировать и переиспользовать. Обученные модели использовать не нужно, код для их обучения можно использовать как подсказку. \n",
    "\n",
    "## ПРАВИЛА\n",
    "1. Домашнее задание выполняется в группе до 3-х человек.\n",
    "2. Домашнее задание сдается через anytask, инвайты будут дополнительно высланы.\n",
    "3. Домашнее задание оформляется в виде отчета либо в .pdf файле, либо ipython-тетрадке. \n",
    "4. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
    "5. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет.\n",
    "6. Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
    "7. Плагиат и любое недобросоветсное цитирование приводит к обнуление оценки. \n",
    "\n",
    "\n",
    "## Часть 1. [2 балла] Эксплоративный анализ \n",
    "1. Найдите топ-1000 слов по частоте без учета стоп-слов.\n",
    "2. Найдите топ-10 по частоте: имен, пар имя + фамилия, пар вида ''профессор'' + имя / фамилия. \n",
    "\n",
    "[бонус] Постройте тематическую модель по корпусу HPAC.\n",
    "\n",
    "[бонус] Найдите еще что-то интересное в корпусе (что-то специфичное для фанфиков или фентези-тематики)\n",
    "\n",
    "## Часть 2. [2 балла] Модели представления слов \n",
    "Обучите модель представления слов (word2vec, GloVe, fastText или любую другую) на материале корпуса HPAC.\n",
    "1. Продемонстрируйте, как работает поиск синонимов, ассоциаций, лишних слов в обученной модели. \n",
    "2. Визуализируйте топ-1000 слов по частоте без учета стоп-слов (п. 1.1) с помощью TSNE или UMAP (https://umap-learn.readthedocs.io).\n",
    "\n",
    "## Часть 3. [5 баллов] Классификация текстов\n",
    "Задача классификации формулируется так: данный фрагмент фанфика описывают какую-то ситуацию, которая предшествует произнесению заклинания. Требуется по тексту предсказать, какое именно заклинание будет произнесено. Таким образом, заклинание - это фактически метка класса. Основная мера качества – macro $F_1$.\n",
    "Обучите несколько классификаторов и сравните их между собой. Оцените качество классификаторов на частых и редких классах. Какие классы чаще всего оказываются перепутаны? Связаны ли ошибки со смыслом заклинаний?\n",
    "\n",
    "Используйте фрагменты из множества train для обучения, из множества dev для отладки, из множества test – для тестирования и получения итоговых результатов. \n",
    "\n",
    "1. [1 балл] Используйте fastText в качестве baseline-классификатора.\n",
    "2. [2 балла] Используйте сверточные сети в качестве более продвинутого классификатора. Поэкспериментируйте с количеством и размерностью фильтров, используйте разные размеры окон, попробуйте использовать $k$-max pooling. \n",
    "3. [2 балла] Попробуйте расширить обучающее множество за счет аугментации данных. Если вам понадобится словарь синонимов, можно использовать WordNet (ниже вы найдете примеры).\n",
    "\n",
    "[бонус] Используйте результат max pooling'а как эмбеддинг входного текста. Визуализируйте эмбеддинги 500-1000 предложений из обучающего множества и изучите свойства получившегося пространства.\n",
    "\n",
    "[бонус] Используйте ваш любимый классификатор и любые (честные) способы повышения качества классификации и получите macro $F_1$ больше 0.5.\n",
    "\n",
    "## Часть 4. [1 балл] Итоги\n",
    "Напишите краткое резюме проделанной работы. Читали ли вы сами Гарри Поттера или фанфики о нем и помогло ли вам знание предметной области в выполнении домашнего задания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nikitos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "import gc\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "SENTS_FILE_NAME = 'sents_source.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_file(name):\n",
    "    with open(name, 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def split_text_into_sent(text):\n",
    "    return nltk.sent_tokenize(text)\n",
    "        \n",
    "def split_file_into_sent(name, save_file_name=''):\n",
    "    lines = load_text_from_file(name)\n",
    "    sents = []\n",
    "    for line in lines:\n",
    "        sents += split_text_into_sent(line)\n",
    "    if save_file_name != '':\n",
    "        with open(save_file_name, 'wb') as f:\n",
    "            pickle.dump(sents, f)\n",
    "    return sents\n",
    "\n",
    "def split_files_in_dir_into_sent(dir_name, save_file_name=''):\n",
    "    sents = []\n",
    "    for file_name in tqdm(os.listdir(dir_name)):\n",
    "        try:\n",
    "            sents += split_file_into_sent(os.path.join(dir_name, file_name))\n",
    "        except Exception as e:\n",
    "            print('-------------------------')\n",
    "            print(file_name)\n",
    "            print(repr(e))\n",
    "            print('-------------------------')\n",
    "            \n",
    "    if save_file_name != '':\n",
    "        with open(save_file_name, 'wb') as f:\n",
    "            pickle.dump(sents, f)\n",
    "    return sents\n",
    "sents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif os.path.exists(SENTS_FILE_NAME) and os.path.isfile(SENTS_FILE_NAME):\\n    with open(SENTS_FILE_NAME, 'rb') as f:\\n        sents = pickle.load(f)\\nelse:\\n    sents = split_files_in_dir_into_sent(dir_name='hpac_source/', save_file_name='sents_source.pickle')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if os.path.exists(SENTS_FILE_NAME) and os.path.isfile(SENTS_FILE_NAME):\n",
    "    with open(SENTS_FILE_NAME, 'rb') as f:\n",
    "        sents = pickle.load(f)\n",
    "else:\n",
    "    sents = split_files_in_dir_into_sent(dir_name='hpac_source/', save_file_name='sents_source.pickle')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unidecode import unidecode\n",
    "import pymorphy2\n",
    "\n",
    "TRAIN_MARKUP_NAME = 'hpac_corpus/hpac_training_128.tsv'\n",
    "DEV_MARKUP_NAME = 'hpac_corpus/hpac_dev_128.tsv'\n",
    "TEST_MARKUP_NAME = 'hpac_corpus/hpac_test_128.tsv'\n",
    "FASTTEXT_TRAIN_NAME = 'FastText/train.txt'\n",
    "FASTTEXT_TEST_NAME = 'FastText/test.txt'\n",
    "FASTTEXT_DEV_NAME = 'FastText/dev.txt'\n",
    "TRAIN_BACKUP_NAME = 'backup/train.pickle'\n",
    "DEV_BACKUP_NAME = 'backup/dev.pickle'\n",
    "TEST_BACKUP_NAME = 'backup/test.pickle'\n",
    "\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "regex = re.compile(\"[A-Za-z:=!\\)\\()\\_\\%/|]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def lemmatize(text, pymorphy=m):\n",
    "    try:\n",
    "        return \" \".join([pymorphy.parse(w)[0].normal_form for w in text])\n",
    "    except:\n",
    "        return \" \"\n",
    "    \n",
    "def clean_text(text):\n",
    "    return lemmatize(words_only(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifiedObject:\n",
    "    def __init__(self):\n",
    "        self.class_name = None\n",
    "        self.id = None\n",
    "        self.text = None\n",
    "        self.embedding = None\n",
    "    \n",
    "class MarkupReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.__class_name_to_id = dict()\n",
    "        self.__id_to_class_name = dict()\n",
    "        self._read_markup(file_path)\n",
    "    \n",
    "    def _read_markup(self, file_path):\n",
    "        self.__df = pd.read_csv(file_path, sep='\\t', header=None, encoding='utf-8')\n",
    "        self.__df.columns = ['Garbage', 'ClassName', 'Text']\n",
    "        #self.__df['ClassName'].str.encode('utf-8')\n",
    "        self.__df['Text'] = self.__df['Text'].map(unidecode)\n",
    "        self.__df['Text'] = self.__df['Text'].map(clean_text)\n",
    "        class_names_set = set(list(self.__df['ClassName'].values))\n",
    "        for i, class_name in enumerate(class_names_set):\n",
    "            self.__class_name_to_id[class_name] = i\n",
    "            self.__id_to_class_name[id] = class_name\n",
    "    \n",
    "    def get_object_count(self):\n",
    "        assert self.__df is not None\n",
    "        return len(self.__df)\n",
    "    \n",
    "    def get_class_count(self):\n",
    "        return len(self.__class_name_to_id)\n",
    "    \n",
    "    def get_object(self, index):\n",
    "        assert self.__df is not None\n",
    "\n",
    "        obj = ClassifiedObject()\n",
    "        obj.class_name = self.__df['ClassName'].values[index]\n",
    "        obj.id = self.__class_name_to_id[obj.class_name]\n",
    "        obj.text = self.__df['Text'].values[index]\n",
    "        return obj\n",
    "        \n",
    "    def make_fasttext_dataset(self, file_name):\n",
    "        assert self.__df is not None\n",
    "        f = open(file_name, 'w')\n",
    "        for i in tqdm(range(self.get_object_count())):\n",
    "            obj = self.get_object(i)\n",
    "            line = '__label__' + obj.class_name + ' '\n",
    "            line = line + obj.text + '\\n'\n",
    "            #print(line)\n",
    "            f.write(line)\n",
    "        f.close()\n",
    "        \n",
    "    def get_text_array(self):\n",
    "        return self.__df['Text'].values\n",
    "        \n",
    "    def get_label_array(self):\n",
    "        return self.__df['ClassName'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved\n",
      "Processed and saved\n",
      "Processed and saved\n"
     ]
    }
   ],
   "source": [
    "def read_markup(file_name, backup_name):\n",
    "    if os.path.exists(backup_name) and os.path.isfile(backup_name):\n",
    "        print('Found backup')\n",
    "        with open(backup_name, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    markup = MarkupReader(file_name)\n",
    "    with open(backup_name, 'wb') as f:\n",
    "        pickle.dump(markup, f)\n",
    "    print('Processed and saved')\n",
    "    return markup\n",
    "\n",
    "dev_markup = read_markup(DEV_MARKUP_NAME, DEV_BACKUP_NAME)\n",
    "test_markup = read_markup(TEST_MARKUP_NAME, TEST_BACKUP_NAME)\n",
    "train_markup= read_markup(TRAIN_MARKUP_NAME, TRAIN_BACKUP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7605/7605 [00:00<00:00, 118269.22it/s]\n",
      "100%|██████████| 60980/60980 [00:00<00:00, 168506.20it/s]\n",
      "100%|██████████| 7679/7679 [00:00<00:00, 96417.46it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_markup.make_fasttext_dataset(FASTTEXT_DEV_NAME)\n",
    "train_markup.make_fasttext_dataset(FASTTEXT_TRAIN_NAME)\n",
    "test_markup.make_fasttext_dataset(FASTTEXT_TEST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.train_supervised(FASTTEXT_TRAIN_NAME, label_prefix='__label__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_prediction = fasttext_model.predict(test_markup.get_text_array().tolist())\n",
    "fasttext_prediction = [pred[0] for pred in fasttext_prediction[0]]\n",
    "fasttext_prediction = [s[len('__label__'):] for s in fasttext_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1(y_true, y_pred):\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022842458232546596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitos/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(calculate_f1(y_true=test_markup.get_label_array(), y_pred=fasttext_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
